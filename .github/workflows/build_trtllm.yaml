name: Build TensorRT-LLM

on:
  push:
    branches:
      - 'main'
      - 'trtllm/ci'
    tags:
      - 'v*'
  pull_request:
    paths:
      - "backends/trtllm"
      - "server/**"
      - "proto/**"
      - "router/**"
      - "launcher/**"
      - "Cargo.lock"
      - "rust-toolchain.toml"
    branches:
      - "main"

permissions:
  contents: read # Required to check out repository.
  id-token: write # Required to authenticate via OIDC.

jobs:
  build:
    runs-on:
      group: aws-highmemory-32-plus-priv
    steps:
      - uses: actions/checkout@v4

      - name: Extract TensorRT-LLM version
        run: |
          echo "TENSORRT_LLM_VERSION=$(grep -oP '([a-z,0-9]{40})' $GITHUB_WORKSPACE/backends/trtllm/cmake/trtllm.cmake)" >> $GITHUB_ENV
          echo "TensorRT-LLM version: ${{ env.TENSORRT_LLM_VERSION }}"

      - name: "Configure AWS Credentials"
        id: aws-creds
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-east-1
          role-to-assume: ${{ secrets.AWS_ROLE_GITHUB_TGI_TEST }}
          role-duration-seconds: 7200
          output-credentials: true

      - name: Initialize Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          install: true
          buildkitd-config: /tmp/buildkitd.toml

      - name: Login to internal Container Registry
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.REGISTRY_USERNAME }}
          password: ${{ secrets.REGISTRY_PASSWORD }}
          registry: registry.internal.huggingface.tech

      - name: Login to GitHub Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # If pull request
      - name: Extract metadata (tags, labels) for Docker
        if: ${{ github.event_name == 'pull_request' }}
        id: meta-pr
        uses: docker/metadata-action@v5
        with:
          images: |
            registry.internal.huggingface.tech/api-inference/community/text-generation-inference/tensorrt-llm
          tags: |
            type=raw,value=sha-${{ env.GITHUB_SHA_SHORT }}${{ env.LABEL }}

      # If main, release or tag
      - name: Extract metadata (tags, labels) for Docker
        if: ${{ github.event_name != 'pull_request' }}
        id: meta
        uses: docker/metadata-action@v5
        with:
          flavor: |
            latest=auto
          images: |
            registry.internal.huggingface.tech/api-inference/community/text-generation-inference/tensorrt-llm
          #            ghcr.io/huggingface/text-generation-inference
          tags: |
            type=semver,pattern={{version}}${{ env.LABEL }}
            type=semver,pattern={{major}}.{{minor}}${{ env.LABEL }}
            type=raw,value=latest${{ env.LABEL }},enable=${{ github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
            type=raw,value=sha-${{ env.GITHUB_SHA_SHORT }}${{ env.LABEL }}

      - name: Build and push Docker image
        id: build-and-push
        env:
          LABEL: "trtllm"
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile_trtllm
          target: ci-runtime
          push: true
          load: true
          tags: ${{ steps.meta.outputs.tags || steps.meta-pr.outputs.tags }}
          platforms: 'linux/amd64'
          build-args: |
            build_type=dev
            is_gha_build=true
            aws_access_key_id=${{ steps.aws-creds.outputs.aws-access-key-id }}
            aws_secret_access_key=${{ steps.aws-creds.outputs.aws-secret-access-key }}
            aws_session_token=${{ steps.aws-creds.outputs.aws-session-token }}
            sccache_bucket=${{ secrets.AWS_S3_BUCKET_GITHUB_TGI_TEST }}
            sccache_s3_key_prefix=trtllm-${{ env.TENSORRT_LLM_VERSION }}
            sccache_region=us-east-1
          cache-from: type=s3,region=us-east-1,bucket=ci-docker-buildx-cache,name=text-generation-inference-cache-${{ env.LABEL }},mode=min,access_key_id=${{ secrets.S3_CI_DOCKER_BUILDX_CACHE_ACCESS_KEY_ID }},secret_access_key=${{ secrets.S3_CI_DOCKER_BUILDX_CACHE_SECRET_ACCESS_KEY }},mode=min
          cache-to: type=s3,region=us-east-1,bucket=ci-docker-buildx-cache,name=text-generation-inference-cache-${{ env.LABEL }},mode=min,access_key_id=${{ secrets.S3_CI_DOCKER_BUILDX_CACHE_ACCESS_KEY_ID }},secret_access_key=${{ secrets.S3_CI_DOCKER_BUILDX_CACHE_SECRET_ACCESS_KEY }},mode=min

  tests:
    needs: build
    runs-on:
      group: aws-g6-12xl-plus-priv-cache
    container:
      image:
        registry.internal.huggingface.tech/api-inference/community/text-generation-inference/tensorrt-llm:${{ env.LABEL }}
      credentials:
        username: ${{ secrets.REGISTRY_USERNAME }}
        password: ${{ secrets.REGISTRY_PASSWORD }}
      options: --gpus all --net=host

    steps:
      - name: Run C++/CUDA tests
        run: /usr/local/tgi/tgi_trtllm_backend_tests

